<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Chapter6.md</title>
  <style>
    html {
      line-height: 1.8;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<p>Chapter 6:</p>
<h1 id="collaborative-work-is-liberating-and-effective">Collaborative Work Is Liberating and Effective</h1>
<h2 id="poetical-philosophy-from-lovelace-to-linux">Poetical Philosophy, from Lovelace to Linux</h2>
<p>&lt;<em>p.121</em>&gt; From the moment she was born, Augusta Ada Gordon was discouraged from writing poetry. It was a struggle against her genetic predisposition. Her father had led by example in the worst possible way, cavorting around the Mediterranean, leaving whispered tales of deviant eroticism and madness wherever he went. He penned epic stanzas full of thundering drama and licentiousness. Lord Byron understood the dangers of poetry. “Above all, I hope she is not <em>poetical</em>,” he declared upon his daughter’s birth; “the price paid for such advantages, if advantages they be, is such as to make me pray that my child may escape them.” Ada, as she was known, failed to make this escape and barely enjoyed the advantages. The poetry she went on to write was beyond even her father’s imaginings.</p>
<p>Likewise hoping that her daughter might avoid the fate of a father who was “mad, bad and dangerous to know,” Ada’s mother, Lady Anne Isabella Milbanke, ensured that she was schooled with precision and discipline in mathematics from her earliest days, and closely watched her for any signs of the troubles that had plagued her father. Lord Byron, abandoning them weeks after Ada’s birth, died when she was eight; his legacy cast a ghostly shadow over her life.</p>
<p>Ada’s schooling marched ever forward, toward an understanding of the world based on numbers. “We desire certainty not &lt;<em>p.122</em>&gt; uncertainty, science not art,” she was insistently told by one of her tutors, William Frend. Another tutor was the mathematician and logician Augustus De Morgan, who cautioned Ada’s mother on the perils of teaching mathematics to women: “All women who have published mathematics hitherto have shown knowledge, and the power of getting, but … [none] has wrestled with difficulties and shown a man’s strength in getting over them,” he wrote. “The reason is obvious: the very great tension of mind which they require is beyond the strength of a woman’s physical power of application.” But Ada was never going to be denied the opportunity to learn about mathematics. Lady Anne was a talent herself, dubbed “Princess of Parallelograms” by Lord Byron. Having managed to outlive him, the desire to expunge in her daughter the slightest genetic tendency for mad genius and kinky sex took precedence over any concerns about Ada’s feminine delicacy.</p>
<p>Married at nineteen years of age, Ada, now Countess Lovelace, demonstrated curiosity and agility of mind that would prove to be of great service to the world. Just a year before her marriage, in 1833, she had met Charles Babbage, a notable mathematician with a crankish disposition (he could not stand music, apparently, and started a campaign against street musicians). Together they worked on plans for the Analytical Engine, the world’s first mechanical computer. It was designed to be a mechanical calculator, with punch cards for inputting data and a printer for transcribing solutions to a range of mathematical functions. Babbage was a grand intellect, with a penchant for snobbery and indifference to many of the practicalities of getting things done. Lovelace was his intellectual equal but arguably better adapted to social life.</p>
<p>Like the proverbial genius, Babbage struggled with deadlines and formalities. When one of his speeches was transcribed for publication in Italian and neglected by Babbage, Lovelace picked it up and translated it. She redrafted parts of it to provide explanations to the reader. Her work ended up accounting for about two-thirds of the total text. This became her significant contribution to the advancement of computing: turning the transcription into the first-ever &lt;<em>p.123</em>&gt; paper on computer science. It became a treatise on the work she and Babbage did together.</p>
<p>There remains some controversy about the extent of Lovelace’s participation in this project, but ample historical evidence exists to dismiss the detractors, not least the direct praise bestowed on her work and intellect by Babbage. Lovelace applied her mathematical imagination to the plans for the Analytical Engine and Babbage’s vision of its potential. She sketched out the possibility of using the machine to perform all sorts of tasks beyond number crunching. In her inspired graphic history of Babbage and Lovelace, Sydney Padua describes Lovelace’s original contribution as one that is foundational to the field of computer science: “By manipulating symbols according to rules, <em>any</em> kind of information, not only numbers, can be operated on by automatic processes.” Lovelace had made the leap from calculation to computation.</p>
<p>Padua describes the relationship between Babbage and Lovelace as complementary in computational terms. “The stubborn, rigid Babbage and mercurial, airy Lovelace embody the division between hardware and software.” Babbage built the mechanics and tinkered endlessly with the physical design; Lovelace was more interested in manipulating the machine’s basic functions using algorithmic formulas. They were, in essence, the first computer geeks.</p>
<p>The kind of thinking needed to build computers is precisely this combination of artistry and engineering, of practical mechanics and abstract mathematics, coupled with an endless curiosity and desire for improvement. The pioneering pair’s work blurred the division between science and art and navigated the spectrum between certainty and uncertainty. Without Babbage, none of it would have happened. But with Lovelace’s predilection for imaginative thinking and education in mathematics, a perfect alignment of intellect allowed for the creation of computer science. Lovelace and Babbage’s achievements were impressive because they challenged what was possible while at the same time remaining grounded in human knowledge.</p>
<p>And beyond all this, Lovelace was a woman. (A woman!) In direct contradiction to her tutors’ warnings decades earlier, Lovelace, &lt;<em>p.124</em>&gt; Babbage wrote, was an “enchantress who has thrown her magical spell around the most abstract of Sciences and has grasped it with a force which few masculine intellects (in our country at least) could have exerted over it.” Lovelace showed it was possible to transcend not only the bounds of orthodox mathematics but also her socially prescribed gender role.</p>
<p>No doubt all this caused Lovelace’s mother considerable worry. The madness seemed to be catching up, much to her consternation. In the years after her visionary publication, Lovelace poignantly beseeched Lady Anne: “You will not concede me philosophical poetry. Invert the order! Will you give me <em>poetical philosophy</em>, <em>poetical science</em>?”</p>
<p>For Babbage, perfect was the enemy of good, and he never did manage to build a full model of his designs. In 1843, knowing that he struggled with such matters, Lovelace offered, in a lengthy and thoughtful letter, to take over management of the practical and public aspects of his work. He rejected her overtures outright yet seemed incapable of doing himself what was required to bring his ideas to fruition.</p>
<p>Lovelace’s work in dispelling myths and transforming philosophy was cut short when she died of cancer aged just thirty-six. Babbage died, a bitter and disappointed old man, just shy of eighty. The first computers were not built until a century later.</p>
<hr />
<p>Technological advances are a product of social context as much as of an individual inventor. The extent to which innovations are possible will depend on a number of factors external to the individuals who make them, including the education available to them, the resources they have to explore their ideas, and the cultural tolerance for the kind of experimentation necessary to develop those ideas. Melvin Kranzberg, the great historian of technology, observed that “technology is a very human activity—and so is the history of technology.” Humans are responsible for technological development but do not labor in conditions of their own choosing. Had Babbage been a bit more of a practical person, in social as well as technological &lt;<em>p.125</em>&gt; matters, the world may not have needed to wait an extra century for his ideas to catch on. Had Lovelace lived in a time where women’s involvement in science and technology was encouraged, she might have advanced the field of computer science to a considerably greater degree.</p>
<p>So too, then, technological developments more generally can only really be understood by looking at the historical context in which they occur. The industrial revolution saw great advances in production, for example, allowing an economic output that would scarcely be thought possible in the agrarian society that had prevailed a few generations earlier. These breakthroughs in technology, from the loom to the steam engine, seemed to herald a new age of humanity in which dominance over nature was within reach. The reliance on mysticism and the idea that spiritual devotion would be rewarded with human advancement were losing relevance. The development of technology transformed humanity’s relationship with the natural world, a process that escalated dramatically in the nineteenth century. Humans created a world where we could increasingly determine our own destiny.</p>
<p>But such advances were also a method by which workers were robbed of their agency and relegated to meaningless, repetitive labor without craftsmanship. As machines were built to do work traditionally done by humans, humans themselves started to feel more like machines. It is not difficult to empathize with the Luddites in the early nineteenth century, smashing the machines that had reduced their labor to automated work. In resisting technological progress, workers were resisting the separation of their work from themselves. This separation stripped them of what they understood to be their human essence. For, whatever the horrors of feudalism, it allowed those who labored to see what they themselves produced, to understand their value in terms of output directly. Such work was defined, at least to a certain extent, by the human creativity and commitment around it. With industrialization and the atomization of craftsmanship, all this began to evaporate, absorbed into steam and fused into steel. Human bodies became a vehicle for energy transfer, a mere &lt;<em>p.126</em>&gt; input into the machinery of production. It gave poetic significance to the term Karl Marx coined for capital: dead labor.</p>
<p>Though the Luddites are often only glibly referenced in modern debates, the truth is that they were directly concerned with conditions of labor, rather than mindless machine-breaking or some reactionary desire to turn back time. They sought to redefine their relationship with technology in a way that resisted dehumanization. “Luddites opposed the use of machines whose purpose was to reduce production costs,” writes historian Kevin Binfield, “whether the cost reductions were achieved by decreasing wages or the number of hours worked.” They objected to machinery that made poor-quality products, and they wanted workers to be properly trained and paid. Their chosen tactic was industrial sabotage, and when their frame-breaking became the focus of proposed criminal law reform, it was, of all people, Lord Byron who leaped to their defense in his maiden speech to the House of Lords. Byron pleaded that these instances of violence “have arisen from circumstances of the most unparalleled distress.” “Nothing but absolute want,” he fulminated, “could have driven a large and once honest and industrious body of the people into the commission of excesses so hazardous to themselves, their families, and the community.”</p>
<p>The historical effect of this strategy has been to associate Luddites forever with nostalgia and a doomed wish to unwind the advances of humanity. But to see them as backward-looking would be an interpretive mistake. In their writings, the Luddites appear more like a nineteenth-century equivalent of Anonymous: “The Remedy for you is Shor Destruction Without Detection,” they wrote in a letter to the home secretary in 1812. “Prepaire for thy Departure and Recommend the same to thy friends.”</p>
<p>There is something very modern about the Luddites. They serve as a reminder of how many of our current dilemmas about technology raise themes that have consistently cropped up throughout history. One of Kranzberg’s six laws of technology is that technology is neither inherently good nor bad, nor is it neutral. How technology is developed and in whose interests it is deployed is a &lt;<em>p.127</em>&gt; function of politics. The call to arms of the Luddites can be heard a full two centuries later, demanding that we think carefully about the relationship between technology and labor. Is it possible to resist technological advancement without becoming regressive? How can the advances of technology be directed to the service of humanity? Is work an expression of our human essence or a measure of our productivity—and can it be both?</p>
<p>Central to understanding these conundrums is the idea of alienation. Humans, through their labor, materially transform the surrounding world. The capacity to labor beyond the bare necessities for survival gives work a distinct and profound meaning for human beings. “Man produces himself not only intellectually, in his consciousness, but actively and actually,” Marx wrote, “and he can therefore contemplate himself in a world he himself has created.” Our impact on the world can be seen in the product of our labor, a deeply personal experience. How this is organized in society has consequences for our understanding of our own humanity.</p>
<p>What happens to this excess of production—or surplus value—is one of the ultimate political and moral questions facing humanity. Marx’s critique of capitalism was in essence that this surplus value unfairly flows to the owners of capital or bourgeoisie, not to the workers who actually produce it. The owning class deserve no such privilege; their rapacious, insatiable quest for profit has turned them into monstrous rulers. Production becomes entirely oriented to their need for power and luxury, rather than the needs of human society.</p>
<p>Unsurprisingly, Marx reserved some of his sharpest polemical passages for the bourgeoisie. In his view, the bourgeoisie “resolved personal worth into exchange value, and in place of the numberless indefeasible chartered freedoms, has set up that single, unconscionable freedom—Free Trade. In one word, for exploitation, veiled by religious and political illusions, it has substituted naked, shameless, direct, brutal exploitation.”</p>
<p>This experience of exploitation gives rise to a separation or distancing of the worker from the product of her labor. Labor power becomes something to be sold in the market for sustenance, confined &lt;<em>p.128</em>&gt; to dull and repetitive tasks, distant from an authentic sense of self. It renders a human being as little more than an input, a cog, a calculable resource in the machinery of production. For those observing the development of the industrial revolution, this sense of alienation is often bound up with Marx’s analysis of technology. The development of technology facilitated the separation between human essence in the form of productive labor and the outputs of that labor. Instead workers received a wage, a crass substitute for their blood sweat and tears, a cheap exchange for craftsmanship and care. Wages represented the commodification of time—they were payment for the ingenuity put into work. The transactional nature of this relationship had consequences. “In tearing away from man the object of his production,” Marx wrote, “estranged labor tears from him his <em>species-life</em>, his real objectivity as a member of the species, and transforms his advantage over animals into the disadvantage that his inorganic body, nature, is taken from him.”</p>
<p>As Amy Wendling notes, it is unsurprising that Marx studied science. He sought to understand the world as it is, rather than pursue enlightenment in the form of spirituality or philosophy alone. He understood capitalism as unleashing misery on the working class in a way that was reprehensible but also as Wendling put it, “a step, if treacherous, towards liberation.” There was no going back to an agrarian society that valued artisan labor. Nor should there be; in some specific ways, the industrial revolution represented a form of productive progress. But how things were then were not how they could or should be forever. Marx’s thinking was a product of a desire to learn about the world in material terms while maintaining a vision of how this experience could be transcended. Navigating how to go forward in a way that valued fairness and dignity became a pressing concern of many working people and political radicals in his time, a tradition that continues today.</p>
<hr />
<p>Tensions between technology and labor—the objective and subjective, the creative and the analytical—are all around us under capitalism. In the digital age, one field is particularly relevant: the &lt;<em>p.129</em>&gt; history of software development. In it, we can see how labor can be both unalienated and productive, and how it can be limited by the imposition of a profit motive. In a world in which digital technology has great potential for helping us organize human activity efficiently and sustainably, the limits placed on it by capitalist modes of production are worth examining.</p>
<p>For much of the twentieth century, computer programming was not the vastly profitable industry that it is today. (Babbage and Lovelace would point out that it was even less profitable in the nineteenth.) The modern industry of computing started out as niche projects in universities and large industrial companies, or as experimental projects by eccentric individuals. Geeks have always loved pet projects. No doubt the parents of Konrad Zuse were thrilled when he built the first modern mechanical computer in their living room in Berlin in 1936. Zuse was known for being “obsessed” with his machines. He labored in conditions described as “almost total intellectual isolation” (it is unclear if this particular biographer is impressed, sympathetic or both). Apolitical by nature, he appeared to suffer World War Two as something of an irritating distraction from his endless tinkering. As Lovelace observed all too well in Babbage, geekery to some extent has always involved a struggle between the antisocial personality traits that tend to drive the pursuit of such projects and the limitations on zeal imposed by the reality of modern society.</p>
<p>By 1953, IBM had built the first electric computer that was mass-produced. In these early days of computing, up until the late 1970s, the relationship between hardware and software was quite different to how we understand it today. Hardware manufacturers would give away software for free. IBM was one such example: it supplied software as a standard extra when it sold its hardware. The software was “free,” meaning without charge, but also in the sense that IBM encouraged experimentation with its products. Users would provide feedback to IBM to help it make changes and fix problems in the system. If anything, users were streets ahead of IBM in improving the software, the hive minds of geeky college students constantly &lt;<em>p.130</em>&gt; frustrated at the plodding pace of the batch-processing-obsessed developers at IBM.</p>
<p>The software IBM distributed was nonetheless subject to formal copyright protection. It was considered a work of authorship, which imposed some limitations on how it could be used. But this was really only a theoretical matter. In practice, professor Eben Moglen describes, “Mainframe software was cooperatively developed by the dominant hardware manufacturer and its technically sophisticated users, employing the manufacturer’s distribution resources to propagate the resulting improvements through the user community.” In other words, it was a world of collaborative innovation, a constant feedback loop. This sits in contrast to modern understandings of property, which see software as a discrete thing to be owned under copyright. “The right to exclude others, one of the most important ‘sticks in the bundle’ of property rights,” Moglen notes, “was practically unimportant, or even undesirable.”</p>
<p>Stephen Levy, in his overly affectionate if thoughtful portrait of this world, <em>Hackers</em>, published in 1986, offers an insight into the nature of collective innovation in the student computer labs at places like MIT. Students would write all sorts of programs and distribute them widely without expecting anything in return. In fact, they would invite collaboration—leaving the first iteration of a program in a communal storage drawer for others to pick up and work on. One computer company ended up using a program written by MIT students—probably the first ever computer game, called Spacewar—as a diagnostic testing tool before shipment, leaving the game on there for unsuspecting customers to stumble across. Spacewar, like all other programs, was a collaborative project; the framework of the code was written by one person, then added to by others, until eventually students had rigged up the computer lab lighting to synchronize with key moments in the game. Here was creative design undertaken collectively.</p>
<p>It was clear to computer manufacturers that collaborative work on software like this resulted in a superior product for users. The community of programmers worked together to address problems &lt;<em>p.131</em>&gt; and needs they had as individuals, writing code to create solutions that were fed back to the manufacturer. The distinction between user and programmer was blurred, even porous. This kind of voluntary collective work was (and to a certain extent still is) known as hacking. Levy describes hacking as pulling things apart, changing parts and seeing how these changes affected the system overall, with the ultimate aim of putting the various parts to optimal use. Hackers possessed a “kind of restless curiosity,” he wrote; their projects were undertaken not necessarily for profitable or constructive goals but for the “wild pleasure taken in mere involvement.” To qualify as hacking, such work would have to be stylish and display technical innovation. The community of users involved the same people who were creating fixes for their problems, resulting in a virtuous cycle of collaborative improvement.</p>
<p>Such a world was not open to everyone. You had to have time, skills and devotion to the craft to succeed. This kind of meritocratic thinking allowed some people to participate in the lab in a way that broke traditional social rules. David Silver, for example, started working in the MIT lab before he had finished school. He began hanging around at just fourteen years of age, a fact many hackers probably failed to notice. What they did notice, however, was his skill with robotics, especially when he managed to build what he called the “Silver Arm”—the first robotic arm controlled by the lab’s computer.</p>
<p>But alongside the capacity of hacker culture to discard social mores that might have prevented a kid like Silver being allowed past the front door of the lab, it cultivated a regrettable indifference to those most held back by such social strictures. Women were the most obvious example. There were very few of them active in the MIT lab. “The sad fact was that there was never a star-quality female hacker. No one knows why.” wrote Levy, with a tin ear.</p>
<blockquote>
<p>There were women programmers and some of them were good, but none seemed to take to hacking … Even the substantial cultural bias against women getting into serious computing does not explain the utter lack of &lt;<em>p.132</em>&gt; female hackers. “Cultural things are strong, but not <em>that</em> strong,” [Bill] Gosper would late conclude, attributing the phenomenon to genetic, or “hardware,” differences.</p>
</blockquote>
<p>This absence of women was partly a function of women being underrepresented in disciplines associated with computing, since such fields were traditionally considered male preserves, including science, engineering and mathematics. But this is hardly the whole story: the reality is that many women, including Lovelace but also countless others, have made significant contributions to the history of computing.</p>
<p>Grace Hopper was instrumental to the field of computer programming, cutting her teeth at Harvard during World War Two and later joining other female colleagues at Pennsylvania State University. Hopper was committed to making computer programming available to non-experts. She worked on pioneering developments such as the use of compilers and the creation of the language COBOL, which remain influential today. Many African American women too did groundbreaking work at NASA, including Katherine Johnson, Dorothy Vaughan and Mary Jackson, whose experiences have been documented in the book (and film) <em>Hidden Figures</em>. These women were known as human computers, tackling enormously complex mathematical tasks and getting very little credit because of their gender and race. Together with colleagues like Margaret Hamilton, responsible for the Apollo on-board flight software around the same time, they performed work that was essential to NASA and the field of computer science more generally.</p>
<p>As the industry began to professionalize—formalizing its status as more elite, more specialized and deserving of more respect—women were pushed out by various gatekeepers. The idea expressed by Levy or Gosper that women were somehow biologically unsuited to this kind of work was therefore not only historically inaccurate, it also demonstrated a lack of understanding among the early hackers of how social expectations function in specific and sophisticated ways. The absence of women and the downplaying of their &lt;<em>p.133</em>&gt; achievements in the field were taken for granted, due to a naive presumption that it was down to something innate or genetic, rather than to external social or political factors that could be changed.</p>
<p>And these prejudices have proven resilient, as shown by the popularity of the “anti-diversity” memo circulated among Google workers in 2017. This memo argued that the gender imbalance in the industry was less a function of unconscious bias than of differences in the distribution of traits between men and women. The hacker and tech communities are not immune to the structural factors that shape the world around them. Hackers have often valued the idea that they are working in a meritocracy, where worth is measured according to objective factors like skill, creativity and effort. But rather than elevating fairness, this has flattened difference. Devotion to meritocratic thinking has made it too easy to ignore how unfairness has been built in to our social systems and structures. Meritocratic thinking claims to be gender-blind, but this has effectively meant being blind to gender inequality.</p>
<p>These are legitimate critiques of the free-spirited culture of collaboration of the MIT lab, and they unfortunately remain relevant today. But without dismissing such concerns, it is still possible to observe in this history the raw material of alternative ways of working and organizing productive labor.</p>
<p>The MIT lab was a hotbed of productive activity, but it was also more than mere number crunching or code writing. Modern hacking, just like the work of Babbage and Lovelace, is at heart a creative pursuit. Levy observed this in transcribing the so-called hacker ethic he observed during the period, which included the declaration: “You can create art and beauty on a computer.” It is tempting to think of computer programming as a left-brain activity, for mathematicians and logicians. But hacking is without question an artistic endeavor. It is based on aesthetics: hacking is the art of finding the most elegant solution to a particular problem.</p>
<p>Donald Knuth figures as something of an elder statesman of computing. First published in 1968, his foundational work is called, tellingly, <em>The Art of Computer Programming</em>. Knuth talks &lt;<em>p.134</em>&gt; about computer programming as both a science and an art. He defines science as “knowledge which we understand so well that we can teach it to a computer.” When we reach the limits of this knowledge, Knuth argues, we turn to art to address the mystery. He writes:</p>
<blockquote>
<p>When I speak about computer programming as an art, I am thinking primarily of it as an art <em>form</em>, in an aesthetic sense. The chief goal of my work as educator and author is to help people learn how to write <em>beautiful programs</em> … when we prepare a program, it can be like composing poetry or music; … programming can give us both intellectual and emotional satisfaction, because it is a real achievement to master complexity and to establish a system of consistent rules.</p>
</blockquote>
<p>Hacking is work that is as much about means as it is about ends, and writing computer programs is an exercise in both art and science; it is both functional and aesthetic labor. Such labor provides the opportunity to contemplate ourselves in a world we have created what Marx called our <em>species-life</em>.</p>
<p>Innovation is not epitomized by some tortured genius working alone or a billionaire who once came up with a clever idea. Some of our most radical new technological developments were a result of teamwork, drawing on multiple people’s varied skill sets. Working in this way is both empowering, as we understand how we contribute to a broader project, but also effective, because this kind of collaboration between diverse minds allows us “to master complexity and to establish a system of consistent rules.” Computing—one of the greatest technological advances in recent centuries—began as a small pocket of sophisticated craft labor practiced in a relatively unalienated manner, while the world of capitalist enterprise carried on all around.</p>
<p>But with the rise of personal computing, many programmers recognized that software could also be a source of profit. Hardware became cheaper and available in the consumer marketplace as the personal computer industry began to grow. IBM “unbundled” is &lt;<em>p.135</em>&gt; products, out of antitrust concerns, selling hardware separately from the provision of software. Other companies began to follow suit. In response to these developments, some programmers began writing software independently for sale, not simply to serve the functional utility of hardware.</p>
<p>With this development of an industry came a critical change in the process of creating software. Proprietary software—software sold as a product, usually independent of hardware—depended on a “closed source” model, whereby the source code was concealed or closed off from the user. Software was sold in an executable format, without disclosing its underlying structure or processes. This design was required by those selling proprietary software to limit the extent to which users could simply copy it, share it and install the program, bypassing the author and payment. It represented a fundamental transformation in the process of software design, from one that transparently served the interests of users to one that served the interests of the owners of companies.</p>
<p>These changes in software production came with legal and political realignments. Copyright law took on new significance. Most famously, this found expression in Bill Gates’s letter to his community of fellow computer hobbyists in 1976. Frustrated with colleagues who took a more liberal attitude to software sharing, Gates exhorted the community to pay up:</p>
<blockquote>
<p>As the majority of hobbyists must be aware, most of you steal your software. Hardware must be paid for, but software is something to share. Who cares if people who worked on it get paid? Is this fair? …</p>
<p>What hobbyist can put three man-years into programming, finding all the bugs, documenting his product and distribute it for free? …</p>
<p>Most directly, the thing you do is theft.</p>
</blockquote>
<p>Gates’s campaign to generate respect for proprietary software was not universally welcomed among the burgeoning community of hackers. Some directly opposed it. But there was no denying that Gates had a very big impact on the world of software design.</p>
<p>&lt;<em>p.136</em>&gt; However, Gates represented only one part of the history of software. As a counterpoint to people like him, other hackers started to think more broadly about the idea of freedom in the context of their work and tried to put this into practice. One of these was Richard Stallman. He began working as a programmer at Harvard in the early 1970s and spent a happy decade of creative software development there with fellow geeks, learning and tinkering. But it was not to last. Stallman was devastated when his community fell apart as programmers were lured into private industry. Fellow developers sold programs to companies, bringing them under the strictures of copyright, making them no longer available for modification or collaborative rewriting.</p>
<p>This experience motivated him to create an entire ecosystem of software that was free of these influences and constraints. He called this system GNU, a word play that defined his program differently to Unix, the other main operating system at the time (GNU stood for: Gnu Not Unix).</p>
<p>The kind of “free” Stallman was thinking about was not a reference to the price of software, though free software was often accessible for little more than the cost of copying and posting the disks. (Today most free software is downloadable without charge.) Rather, people like Stallman were concerned with freedom in the developmental environment. Stallman describes it as free as in “free speech,” not as in “free beer.” “Software sellers want to divide the users and conquer them, making each user agree not to share with others,” Stallman wrote in his manifesto, first published in 1983. “I refuse to break solidarity with other users in this way.” Stallman called for volunteers to assist in the project, and many joined him.</p>
<p>The kind of freedom these hackers were after concerned the conditions of labor and the liberty to collaborate. It was about the promise of creative work without the necessity of working for wages. Again, such collaboration was not possible for everyone, as partic pants had to have the economic means and be taught certain skills to be involved. But it was, at its core, the beginning of a movement that &lt;<em>p.137</em>&gt; aimed to dismantle the alienation associated with proprietary software production and use.</p>
<p>In resisting the idea of closed, proprietary software by means of open, liberated software, these hackers shared a common objective with the Luddites over two centuries earlier. The Luddites struggled against the attacks on the integrity of their work that were occasioned by industrialized production techniques. The hackers similarly refused the prospect of being made to perform shoddy work in closed corporate environments, and they devoted themselves to undermining the power of the proprietary software system. Luddites broke frames; hackers wrote open source code and continue to do so today.</p>
<p>One of the most interesting strategies in this movement was its use of the law. Stallman worked with lawyers to devise legal protection for free software development. He published the code for GNU programs, still as a species of copyright but under a form of license known as the General Public License (GPL). The GPL is what is called a “permissive” license. Its stated aim is “to guarantee your freedom to share and change all versions of a program—to make sure it remains free software for all its users.” It essentially serves to protect the openness or liberated nature of a piece of software by ensuring the code is available to all users into the future. The terms of the GPL permit sharing, modification, hacking, and even combining GNU programs with proprietary products for sale. The only requirement is that <em>all</em> of the subsequent source code be also released under the GPL. This feature of the GPL is often labeled “viral”: it serves to protect free software from proprietary expropriation but also to build and expand the digital commons by ensuring that future programs using free software are also publicly available. In short, the GPL takes the traditional view of copyright and inverts it.</p>
<p>The object of the GPL, together with other permissive licensing arrangements inspired by its example, is to reverse the rights of the author and protect the freedom of the user. Together these licenses are referred to as “copyleft.” Copyleft is often accompanied by the tag “all rights reversed,” an inversion of the traditional notion of &lt;<em>p.138</em>&gt; copyright as “all rights reserved.” It broke down the distinction between author and user, allowing these roles to be much more fluid than could ever be possible with closed source, proprietary software.</p>
<p>Stallman may have been the first to think about these issues and give them legal expression. But he was not the only one. He was representative of a broader and growing movement of hackers geeks and hobbyists, all tinkering with the same early computer programs and working together to improve them all over the world. The best example of this was the development of the most complex part of any operating system, the kernel, which acts as the facilitator between the hardware and software applications. Stallman had delayed building it: constructing a free kernel was going to be a tricky, time-consuming task of monumental proportions. It was the last and missing piece in Stallman’s vision of a complete set of free software.</p>
<p>In 1979, one of the main kernels available at the time, Unix, changed its licensing arrangements to prohibit users from reading or modifying the source code. The decision to close the code for Unix was made by AT&amp;T (who owned the code) when an antitrust ruling expired that had prevented them from selling the product commercially. AT&amp;T got down to the business of selling Unix, and, as such, it was no longer an open source product available for people to tinker with and modify. With Unix out of reach for hackers, an academic named Andrew Tanenbaum, at the Free University in Amsterdam, created a mini-kernel for his students to learn from, called Minix. The source code was freely available and quickly became popular among the growing community of hackers with access to more widely available computers, looking for alternative programs to innovate, play with and improve. However, Tanenbaum was not interested in taking the many suggestions he received on how to improve Minix, as it was designed as an educational program, and he wanted it to be kept simple. The hackers among Minix users, constantly itching to test the potential of their computers, began to feel frustrated.</p>
<p>One such hacker, Finnish computer science student Linus Torvalds, resolved instead to just start from scratch and create a &lt;<em>p.139</em>&gt; new kernel. He called it Linux. In part because of the ambitious size of the project, but also the school of hackers in which he swam, Torvalds made the source code of Linux freely available. He posted his project to the community board and invited feedback. Shortly after, Torvalds licensed it under the GPL, as Stallman had done. Linux became another foundational part of the digital commons of free software. It supplied the missing piece in Stallman’s project of a complete operating system, ultimately known as GNU/Linux.</p>
<p>This collective creation was the product of a community devoted to sharing and collaborating. Torvalds could have taken a closed software path for Linux. He could have kept the source code to himself, working with others to refine his product and take it to market. It is easy to imagine that this would have proven highly profitable for him personally. Yet, interestingly, he described this choice in the following terms: “Making Linux freely available is the <em>single</em> best decision I’ve ever made.” Torvalds understood the design advantages of free software, the most obvious of which is that people provide feedback on the features they want, so the product is designed for and services them. But there are other important structural features in this design process. Many minds work on finding bugs and fixing problems, meaning that improvement happens at a rapid rate. Consumers of the product use it, find bugs and report them, launching a new cycle of production. Such an environment is reminiscent of the early days at IBM and the MIT computer lab. It was a progressive adaptation of the original hacker style.</p>
<p>Linux, with the help of the Internet, was worked on round the clock in a multinational project of collaborative improvement. The result was that Linux developed at an astonishing pace; the sheer number of people and hours devoted to its expansion and improvement meant that it overcame design inferiorities, to a high standard. “Everybody puts in effort into making Linux better, and everybody gets everybody else’s effort back,” Torvalds explained. “And that’s what makes Linux so good: you put in something, and that effort <em>multiplies</em>.”</p>
<p>&lt;<em>p.140</em>&gt; The outcome of the free software movement was an entire software ecosystem, produced in conditions of openness and freedom protected by copyleft licensing. Many thousands of users, both seasoned hackers and everyday people, have contributed to creating GNU/Linux. The project involved countless hours of human work, contributed usually for free, coordinated over decades across several continents. Many thousands of other programs are also licensed under the GPL, often produced in similar circumstances. The resulting body of software is considered superior in design and performance to many commercial products, even by its competitors. Linux, for example, has gone on to become the kernel that forms the backbone of many operating systems for both supercomputers and android consumer products. While free software programs are used in many consumer products sold in the marketplace, their component parts are available for zero or very limited cost. This represents an objectively stunning achievement, not only in terms of the quality of the output but also logistically.</p>
<p>The hacker ethic found an adversary in the development of the proprietary software industry. But it was not displaced. It took on new life in hackers who worked on projects motivated by fun, by the sheer joy that flows from creating something that is useful. It serves as a living, breathing refutation of Bill Gates’s haughty assumption that creativity and ingenuity are motivated by the prospect of material gain. It undercuts orthodox economists’ understanding of the free rider problem and lays bare mistaken beliefs about the selfishness of human nature. It contradicts the idea that only experts or the highly trained can craft beautiful things, and it suggests that the concept of a single author may not be the best way to structure work that advances humanity.</p>
<p>And, like many movements that have come before it, the free software movement demonstrates directly the power of working as a collective and orienting the resulting products to the service of users. The diversity of contributors, much like the varied skills of Lovelace and Babbage, propels us to new heights in technological terms. The software that has been produced by the free software &lt;<em>p.141</em>&gt; movement is transparent, accountable and adaptable. It also challenges the legitimacy of the enormous profits raked in annually by an entire industry.</p>
<hr />
<p>Volkswagen probably never expected to be found out. The car manufacturer admitted in late 2015 to systematically using software to cheat environmental regulation. In testing, everything had looked fine. But once on the road, their cars performed better—and the emissions levels were well above legal limits. The software in the car, it seems, was designed to pick up when it was being tested and switch on emission-reducing technology. In normal conditions, the cars roared along with excellent acceleration at hefty speeds, spewing diesel fumes into the atmosphere.</p>
<p>Modern cars containing computer technology often have as much technological sophistication as the average smartphone. They can contain nearly twice the amount of code as you would find in Facebook or the Large Hadron Collider, for example. With such large volumes of code comes a greater risk of vulnerability or error, or potentially deception. They are a good example of how we put vast amounts of computing power into everyday objects and rarely consider the risks associated with allowing the underlying source code to remain secret.</p>
<p>For the last few years, two security researchers, Charlie Miller and Chris Valasek, have taken to hacking various cars, scanning for weaknesses in the code that might allow them to control the vehicle remotely. Miller and Valasek were able to hack into a Ford and a Toyota by “plugging into a diagnostic port that could control the vehicle’s steering and speed.” As time went on, their hacking gained sophistication, eventually going wireless. Andy Greenberg described driving a Jeep Cherokee as the guinea pig in a troubling experiment:</p>
<blockquote>
<p>I was driving 70 mph on the edge of downtown St. Louis when the exploit began to take hold.</p>
<p>Though I hadn’t touched the dashboard, the vents in the Jeep Cherokee &lt;<em>p.142</em>&gt; started blasting cold air at the maximum setting, chilling the sweat on my back through the in-seat climate control system. Next the radio switched to the local hip-hop station and began blaring Skee-lo at full volume. I spun the control knob left and hit the power button, to no avail. Then the windshield wipers turned on, and wiper fluid blurred the glass …</p>
<p>I mentally congratulated myself on my courage under pressure. That’s when they cut the transmission.</p>
<p>Immediately my accelerator stopped working. As I frantically pressed the pedal and watched the RPMs climb, the Jeep lost half its speed, then slowed to a crawl. This occurred just as I reached a long overpass, with no shoulder to offer an escape. The experiment had ceased to be fun.</p>
</blockquote>
<p>Hackers in the security field actively look for these weaknesses in software; they are part of the rush to find zero-day vulnerabilities. As discussed earlier in relation to the WannaCry worm, there is a whole market in finding these vulnerabilities and then notifying the company so it can fix them. In many ways, this resembles a much more expensive and inefficient version of a free software project—a roundabout way of getting to the same point. The problem is not just that these vulnerabilities affect software’s capacity to do its job properly. Nor is it that the NSA jeopardizes our collective safety (discussed in chapter 3) by accumulating these vulnerabilities as a stockpile of digital weaponry. These vulnerabilities can also be created by companies seeking, for example, to evade legal regulation. Closed source code allows businesses to hack their way around democratically made laws.</p>
<p>Volkswagen’s cheating on emissions testing carried on undetected for a considerable period of time, because, just like with the Jeep and most other consumer products that run software, there is no way to read the source code. It is closed source, protected by copyright, and there is no requirement that a company reveal it. It is deeply ironic that the Environmental Protection Agency had actively resisted attempts to exclude this kind of software from copyright protections and make it available for scrutiny. The EPA had argued that making the source code available meant that consumers could potentially &lt;<em>p.143</em>&gt; tinker with it, to improve the car’s performance in violation of environmental regulations. But, unsurprisingly, it turns out that it is not the everyday hacker whom we should fear: businesses are the ones that benefit from this kind of secrecy.</p>
<p>The experts and the media were quick to pillory the company and its conduct as “scandalous,” “unusual” with “a degree of complacency and pomposity,” an “accident waiting to happen.” The company was founded by the Nazis, we were reminded, as though this somehow, however absurdly, explained its bad behavior. But Volkswagen is hardly an outlier. The company’s conduct was unlawful, but, on another reading, it was simply exploiting a comparative advantage, externalizing a negative, disrupting a price indicator. Economists are coy about using the proper language for such behavior, which is a form of mundane deception that is the logical incentive of market economics. Why wouldn’t companies manipulate software in this way? With their code under the cloak of secrecy, there were minimal prospects of being caught. It is the sensible response to the regulatory intervention overlaid on market forces. “Mundane” now appears to be the most appropriate adjective, given that several other car companies have since come under scrutiny for doing the same thing.</p>
<p>It gives us pause for thought about what might be next on this rickety roller coaster of predatory capitalism. Many consumer products are now reliant on software that is closed source. Expensive medical equipment that falsely reports good results or recommends unnecessary diagnostic testing? Airplanes with bugs in their navigational software that no one noticed, until, say, a plane goes missing? The problem is not merely cheating, which is congruent with the nature of capitalism. The problem is that this kind of reliance on software that is kept secret is, as a matter of design, unsafe. Like a quick paint job before an auction, the cloak of secrecy around software codes conceals the cracks in the foundations. It becomes a dangerous building material.</p>
<p>Free software, with its sprawling and at times chaotic approach to development, has been compared unfavorably to proprietary &lt;<em>p.144</em>&gt; software, with its sheen of corporate organization and respectability. But one of the key lessons that emerged in the development of Linux—later dubbed “Linus’s Law”—is that “given enough eyeballs, all bugs are shallow.” In other words, the more people are looking at the code, the quicker its problems are spotted and solved. Open source development—by which I mean projects in which the source is open and available for anyone to see—can be time-consuming and anarchic, but it is also effective. Cheating of the kind done by Volkswagen would be hard to conceal if there were more eyeballs on the code. So too would any design flaws.</p>
<p>A similar approach gave rise to the Agile Manifesto, written by a number of software developers in 2001. Agile management focuses on creating working products quickly and prioritizes cooperation between individuals and teams over formal processes. First regarded as a somewhat hippie approach to organizing production, it has gone on to represent a revolutionary moment in the history of management. Agile is about placing importance on engaging workers (or resisting alienation); it values collaboration (rather than competition); it is about giving people the chance to self-organize (rather than suffer micromanagement). Placing importance on feedback and transparency, often in person, has meant that Agile has become a more accountable form of management that is driven from the bottom up. As a technique, it has transformed software development and is associated with productivity and excellence.</p>
<p>This style of management has its downsides, particularly if implemented thoughtlessly—Facebook’s now-discarded mantra of “move fast and break things” comes to mind. It also potentially papers over workplace inequalities with managerial egalitarianism. Similarly, there is a history and tendency among advocates of open source software (as opposed to free-as-in-freedom software) to find ways to make this approach to software production compliant with and complementary to the market. But Agile approaches to both industrial organization and open source design can still produce radical logical outcomes or, as Wendy Liu argues about open source, act as “gateways to a more radical politics” that &lt;<em>p.145</em>&gt; “pushes for the decommodification of not just information but also the material resources needed to sustain the production of information.” These processes and ways of working have the potential to encourage engagement and undermine the exploitative nature of waged work, while building the capacity to generate possible alternatives.</p>
<p>The problem is that computing has become structured in a way that is fundamentally undemocratic. Our whole relationship with personal technology has been organized around the needs of copyright law. That is to say, software has become commodified; the design process serves the interests of the market rather than the interests of the user. This commodification has been the driving force behind the growth of software companies: it locks users into a technological relationship that serves the interests of profit maximization and structures productive capabilities to protect this state of affairs. There is no way for the average consumer to know what their computer, and most of the software on it, is doing. And even if the consumer spots a hazard or a way to do something better, she has no opportunity to communicate this back to a community of users, let alone modify the program. A user can only tell the company and hope they fix it. (Or, less generously, she can profit from companies by selling them zero-day vulnerabilities.) Users are essentially forced onto a one-way superhighway, from producer to consumer, with no opportunity to reverse the direction of engagement. These days, that is not just a matter of getting a program to run on a laptop. As software becomes integrated into more products—as we build the Internet of Things—from cars to fridges to medical equipment and even public infrastructure like mass transit, the potential dangers of keeping this software closed increase.</p>
<p>These are serious and frightening design problems, but they also result in an immense waste of human potential. It is not just that companies do not invite feedback on their software or input from users on their design. Their objective, the purpose of their software, is not to service the user. Their primary goal is to retain control of that software. They want to control who uses it (that is, only &lt;<em>p.146</em>&gt; paying customers). Proprietary software design makes a fetish of creativity—turning it into something abstract and commodified, geared to the purpose of money-making, rather than a collective or public good.</p>
<p>Proprietary software companies also want to remain the centralized force that dictates how the software develops, which requires the people who use their software to remain ignorant. They want users to feel unable to change their experience of using their product. They want troubleshooting to be something that only a Genius Bar can do. They cultivate a sense of helplessness. This kind of supplicant fatuity ultimately serves their bottom line.</p>
<p>This is not a conspiracy theory. Microsoft developed a corporate strategy that sought to promote this exact psychology in its users. Microsoft generated what it called FUD (fear, uncertainty and doubt) in its customers. It did this in a variety of ways, intended to frighten them into using their software on the assumption that it was more reputable and stable. Such tactics included confected pop-up boxes with red warnings when users sought to download or use competing products. The senior vice president at the time, Brad Silverberg, referred to this strategy in a 1992 email, revealed in a lawsuit against a competitor that sold a product in competition with Microsoft’s MS-DOS. “What the [user] is supposed to do is feel uncomfortable,” wrote Silverberg, “and when he has bugs, suspect that the problem is [the competing product] and then go out to buy MS-DOS.”</p>
<p>Stallman has spoken about how proprietary software seeks to make users “hopelessly dependent” and discourages sharing, with the constant threat of imprisonment. It recalls Marx’s description of the experience of labor in the wake of the industrial revolution. The estrangement of labor from production is ultimately one that relies on limiting people’s potential.</p>
<blockquote>
<p>Labour produces for the rich wonderful things—but for the worker it produces privation. It produces palaces—but for the worker, hovels. It produces beauty but for the worker, deformity. It replaces labor &lt;<em>p.147</em>&gt; by machines—but some of the workers it throws back to a barbarous type of labor, and the other workers it turns into machines. It produces intelligence—but for the worker idiocy, cretinism.</p>
</blockquote>
<p>Software design in closed environments acts as a brake on human potential in order to sustain the subjugation of technology to commodified form. It keeps users of software benighted, actively denying them the opportunity to self-educate, out of fear as to how this might affect their profitability. It is an enormous squandering of possibility that take place due to the subordination of software development to shareholder value.</p>
<p>This is not just a shame from a moral perspective. It also represents a power dynamic at play that governs the integrity of our digital systems. Only certain kinds of people think they should get to decide the direction of development of key software programs, including programs that people are dependent on in many aspects of their lives. The public is asked to trust that there is no mistake or deception in their millions of lines of code. The public is also asked to expect that the software they buy will be useful to them, even though it is designed to maximize profit, not utility. Time and again, companies demonstrate that they are not to be trusted and that, without proper regulation, they cheat, undermine their competitors and show disrespect for public resources.</p>
<p>The history of software development provides an insight into some of the biggest barriers to exploring human potential in the future. The proprietary software industry actively sought to resist work being done in an open source format. It blocked paths for collaboration and experimentation; it stood in the way of orienting the collective human brain toward problems people want to solve; instead, it devoted resources to chasing profits. It prefers to risk vulnerabilities, and subsidize the cowboy market in finding and selling them back to companies, rather than open their code to scrutiny. If we apply professor Stafford Beer’s maxim that “the purpose of a system is what it does,” then what copyright and closed source software do is make money for entrepreneurs &lt;<em>p.148</em>&gt; using a substandard system of production. The proprietary software industry has produced software that does not serve its users or their collective purposes; rather, it serves the owners of that software.</p>
<hr />
<p>It is important to imagine a world outside these modes of thinking, because the price we pay for subscribing to the old assumptions is high. With fewer people attempting to solve a problem, seeking creative ways to do something better, or motivated to try something different, the capacity to innovate will be inherently limited. Ada Lovelace was fortunate to have a mother who was determined to teach her math. Without this, she might have been confined to the life of a society woman—and we would have never known as much as we do about the earliest developments in computing. The problem is not that she stepped outside of her socially prescribed role; it is that there were not more people able to do the same.</p>
<p>Limitations in the proprietary software industry may be overcome with an abundance of money and a roomful of highly trained coders. Open source projects are not inherently better; they regularly contain design flaws and often fail to get off the ground as projects. But open source software has better prospects of being effective. Open source software trends toward accountability. Open source software is more likely to service users and come up with new ways to solve problems. Open source software does this because it is serving the people looking at and using the code, not the owners of a corporation, in a manner that is transparent. The best kind of regulation comes through transparency, through which all bugs become shallow. Proprietary software puts profit before safety, efficiency and the public welfare. Open source works because it allows the sunshine to kill all the bugs.</p>
<p>Collaborative innovation means more people working together to find and solve problems and more people benefitting from a better program. It prioritizes the integrity of the product over the need to make money and resists the shoddy workmanship that comes with market incentives. It involves a diversity of viewpoints and a variety &lt;<em>p.149</em>&gt; of skills applied to the necessary tasks, encouraging us to rethink traditional understandings of how innovation happens, which have tended to lionize individual genius. It encourages us see how a motley range of people contributing to poetical science and poetical philosophy can generate technological progress. Human beings often do their best work when they are able to produce themselves intellectually but also actively, and, in doing so, contemplate themselves in a world they have created.</p>
<p>We need to make the open source software movement dangerous again. The free software movement attracted the wrath of all the right people, it created the possibility of building technology differently, and we need to reproduce the radical ways in which that movement approached the idea of work and objectives of production. This will require us to cultivate a diverse and inclusive workforce making open source software whose contributions will not be exploited but properly valued and supported as a contribution to the common good. This is especially important as many people who make software are not doing so in conditions of freedom or even in gentrified settings like Silicon Valley. Producers of software are working for pay in all sorts of places, from Kerala to Shenzhan, as well as often contributing to open source projects in their spare time. If we can find common ground in an understanding of the limits of proprietary software and the potential of free software, we have the potential to organize a whole new cohort of workers around a goal that challenges some of the key tenets of capitalism.</p>
<p>The gods treated Babbage cruelly by, as one historian put it, “having bestowed upon him a vision of a computer, without granting him the tools—technological, financial and diplomatic—to make his dreams come true.” We now inhabit a world where Babbage’s dreams are being realized, and Lovelace’s logic has taken root in the minds of many. Their experiments combining poetry and mathematics, imagination and practicality, were the beginning of a field that is changing our society in all sorts of radical ways. But there are limits on this kind of collaborative human potential, and they are ones that are imposed by capitalism. We need to work &lt;<em>p.150</em>&gt; toward creating conditions in which those limits no longer apply. In imagining what this might look like, we need not begin from a blank slate or concocted utopias, severed from the past. Here, in this specific history of the industry, we see the seeds of alternative ways of working that are fulfiling and liberating; we glimpse the possibilities of production outside the marketplace.</p>
</body>
</html>

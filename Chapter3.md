Chapter 3:

# Digital Surveillance Cannot Make Us Safe

## Policing Bodies and Time on London's Docks

<*p.39*> London in the 1790s was a filthy and bustling metropolis of empire. Maritime traffic up and down the Thames was growing at an astonishing pace, with riches plundered from the colonies winding their way toward markets through this central artery of the city. In a bid to speed up the process of docking and unloading, the West Indian merchants pooled their funds to build a wet dock on the Isle of Dogs in the east of the city, freeing them from dependence on the tides. But this logistical improvement exposed another feature of industrialization. The historian Peter Linebaugh has written about how workers in manufacturing and production houses often supplemented their wages (if they were lucky enough to be paid any) by pilfering stock from their employers. It was customary to do this, especially at a time when workers were testing the limits of the idea of wage labor--a distinct change from their previous ways of life as artisans or peasants. Linebaugh documents social and even judicial recognition of this practice, whether it was warehouse laborers drinking the rum maturing in the storehouses, a printer retaining a copy of every book he assembled, or shipbuilders taking leftover timber for their own use. It is not unlike how a contemporary retail worker might be given a discount on clothing to wear during her shift, or a café waiter given excess pastries at the end of the day. Or <*p.40*> perhaps like start-up employees owning a share in the company as part of their salary. In the London of those days, this culture represented "a kind of 'collective bargaining' over the materials of production," Linebaugh argues.

Among the merchants, however, it was increasingly thought of as theft. The construction of the wet dock put the river workers under increased scrutiny--the process of unloading sugar and other commodities could now be carefully tracked and measured between when it arrived on the ship and when it was eventually sold. The merchants wanted the tradition of workers helping themselves to be stamped out: they were to work as wage laborers paid for their time, and nothing more. But it was a slog getting the slow and unwieldy arm of the law to enforce this.

Until that point, London had no police force. While constables and night watchmen existed in various areas, these bodies were diffuse, unruly, unprofessional, underpaid (if paid at all) and often corrupt. There was no professional body of officers under state control, authorized to use force.

In a direct attempt to protect their property and corral the dockers to conform to the relatively novel strictures of wage labor, John Harriott, a magistrate, farmer and businessman, came up with a plan for a professional police force. He was not the first to imagine such a thing, but his proposal was nonetheless unprecedented. He teamed up with Patrick Colquhoun, a merchant and statistician of repute, and Jeremy Bentham, the famed utilitarian philosopher. Colquhoun and Bentham brought their prestige and political judgment to Harriot's bold ideas, and together these men sought to design organs of civil government that could address the challenges of the industrial age. They wanted to figure out how collaboration among the propertied classes would best allow them to maximize the opportunities presented by the novel and developing system of capitalism.

Specifically, this resulted in a proposal to the merchants' committee to fund an experiment, which was duly accepted. The Marine Police Office opened in 1798 as a kind of pilot program. Officers were paid and uniformed, and their job was to watch over the wet <*p.41*> docks. They supervised the workers and kept an eye on the ships and their cargo. They enforced working hours and were even responsible for paying out wages. When they encountered misbehavior, they did what was necessary to bring the errant workers before a magistrate.

It was a raging success, significantly reducing the merchants' losses for a small price. Harriott congratulated himself for "bringing into reasonable order some thousands of men, who had long considered plunder as a privilege." Working their magic in a way that would impress even the toughest political lobbyist, Harriott, Colquhoun and Bentham managed to convince Westminster to support the project, and the Marine Police Office came under state authority two years later. Not only did the merchants have a police force, it was now funded by the public purse.

Colquhoun wrote about his experiment on the River Thames extensively, keen to export his model around the world. His vision of policing was deeply bound to economics, and he saw the urgency of promoting this model as the industrial revolution transformed material relations. "Police in this country may be considered as a new Science ... in the Pʀᴇᴠᴇɴᴛɪᴏɴ and Dᴇᴛᴇᴄᴛɪᴏɴ ᴏғ Cʀɪᴍᴇs," he declared a few years later in his treatise on the topic. "Under present circumstances of insecurity, with respect to property, and even life itself, this is a subject that cannot fail to force itself upon the attention of all."

The eighteenth century was a time in which the contours of public life and governmental authority were being defined in ways that would last for centuries. How best, then, to inculcate a culture of compliance with this new way of doing things? "It is the dread of the existing power of immediate detection, and the certainty of punishment as the consequence of this detection," wrote Colquhoun, "that restrains men of loose morals from the commission of offences." Surveillance, plus the spectacle of force, offered an economical and effective way of maintaining order. It is not walls, locks or bars that prevent crime, Colquhoun argued; instead, "restraints are only to be effected by the strong and overawing hand of power." He was <*p.42*> describing a tactic that would be deployed by the powerful in our own era: surveillance as a method of social control.

Colquhoun determined that the conversion of river workers into a disciplined, regulated class of wage laborers could only be achieved through moral transformation. This transformation involved the criminalization of idleness and a revision of the sense of injustice that made workers feel entitled to a decent share of the fruits of their labor. It involved the pursuit of order, and faith in the state to protect people from the threat of disorder. Without an organized defense of private property, the nascent capitalist system would inevitably fall into disarray. This requires a certain form of subordination. "Civil government," observed the ideological father of capitalism, Adam Smith, in his *Wealth of Nations*, "so far as it is instituted for the security of property, is in reality instituted for the defense of the rich against the poor, or of those who have some property against those who have none at all." Inequality has always been the raison d'être of the police, to both preserve it and protect people from it consequences. Their role is both paradoxical and self-serving: to create crime (by defining its meaning in practical terms) while also preventing it. For this reason, the establishment of the Marine Police Office was a milestone in both "metropolitan policing and in the history of the wage-form." In this moment, we can see the theory of class division under capitalism put into practice: collaboration between property owners to direct the power of the civil state for their common interest against the poor.

In 1829, Home Secretary Robert Peel established the Metropolitan Police. His eponymous "Bobbies," patrolling the streets of London, were the climax of a process begun over three decades earlier by Harriot, Colquhoun and Bentham. Peel himself had learned the importance of preserving social order during his time trying to manage the colonial occupation of Ireland, and he therefore understood the value of a professionalized force. The model proved useful in many other sites of urban enterprise, including the newly formed United States of America, where immigration and industrialization were creating social and political havoc. The elite of America <*p.43*> faced familiar problems as a settler-colonial state reliant on slavery. American capitalism needed organized civil institutions capable of clearing the land of its original inhabitants to then be tilled by slaves. Newly formed police forces proved crucial in the execution of the task. The scholar Alex Vitale summarizes it neatly: "The origins and functions of the police are intimately tied to the management of inequalities of race and class."

Why is it important to remember this? The history of policing helps us understand some key aspects of statecraft in the digital age. Like their historical counterparts, modern police understand the power of surveillance in enforcing a form of discipline. Safety on the London docks was defined as preserving social order and disempowering the lower classes, rather than reducing harm. Our current form of civil government similarly classifies citizens as an innately unruly mob, to be blamed for their miseries and disciplined into accepting the laws of the market. Law enforcement and intelligence agencies are incorporating technology into their arsenal in ways that would be familiar to those working the docks under the watch of the newly appointed police. As budgets for these agencies continuously grow, out of all proportion to the risks to public safety they supposedly guard against, this history helps explain the phenomenon.

And this tells us something about the idea of public safety, how we understand it and who defines it. By allowing law enforcement and intelligence agencies to tell us what it is to be safe, we allow them to use technology in increasingly oppressive ways. It also means we ignore the potential for technology to truly reduce harm and violence. Digital technology creates the capacity to improve public security and serve the public good, but not if we allow it to be abused by an authoritarian state.

---

We know about the sheer enormity of the American surveillance state because of Edward Snowden. In 2013, he leaked documents that showed how digital surveillance is a cheap and effective method of keeping tabs on local and foreign populations. PRISM, for example, which gathered metadata from an array of major <*p.44*> companies, cost the relatively tiny sum of $20 million annually. Yet the capacity of the National Security Agency verges on the incomprehensible: in 2012, it was processing more than 20 billion communication events from around the globe (both on the telephone and the Internet) every day. The approach was summarized in one slide from an internal presentation: "Collect it All; Process it All; Exploit it All; Partner it All; Sniff it All; Know it All." It aspired to total informational awareness.

The monstrous scale of the ambition and practices of the NSA can make it difficult to make sense of its collective purpose. The agency struggled to analyze the tide of information it gathered every day; it encountered problems in merely storing it. A key challenge was that this kind of surveillance created a lot of noise but not many signals. But the Snowden documents also demonstrate that, in spite of these practical hurdles, there remained a coherent and long-term understanding of how it all fit together. In a candid presentation, an officer identified three factors that motivate the NSA and its surveillance programs: "National Interests, Money and Egos." American capitalism gained influence and profits from dominating the Internet during its early stages of development. This kind of technological project--total surveillance of communication--was part of an effort by the United States to "maintain its grip on the world," as journalist Glenn Greenwald put it. It recalls Marx's description of the modern state as "a committee for managing the common affairs of the whole bourgeoisie."

The NSA and their fellow intelligence partners in the Five Eyes (the name given to the intelligence alliance between the United States, Canada, the UK, Australia and New Zealand) were less than thrilled about the Snowden revelations, but a tacit understanding by the general public of the existence of this capability has had useful consequences for law enforcement and intelligence agencies. Multiple studies confirm that the idea of government surveillance profoundly impacts how we conduct ourselves in digital spaces, narrowing discussions online and reducing engagement with reading material perceived as contentious. Our liberties become conditional <*p.45*> on their good graces. It is as though we are being disciplined into a new way of living, where any experimentation or whim, any deviation or mistake, exists under a threat of force. Old ways of working, communicating or engaging socially take on a new hue of fear, a watermark of pervasive surveillance. It is not unlike, we might imagine, how it felt to be unloading cargo from the Thames as part of the suffocating new category of waged laborer, under the watchful eye of the river police.

This misappropriation of power is not, however, some overzealous aberration. The NSA's surveillance capacity was relatively low-cost and efficient but only because it was predicated on specific kinds of political cooperation. Paul Ohm talks about how digital technology is creating a "database of ruin," where databases will eventually reach a point where they connect every individual to at least one closely guarded secret. This database of ruin is being built not by agencies themselves but by technology capitalism. Intelligence and law enforcement agencies ride on the coattails of the work done by companies to map our abstract identities, discussed in the preceding chapter. British police have paid data brokers to help them profile convicted criminals and estimate their chance of recidivism, and American police have used data from ancestry websites to identify people suspected of a crime. The Snowden documents show how state surveillance programs like PRISM functioned by tapping into the data flows generated by private businesses. The state obtains access to our personal spaces because technology capitalism has already beaten a path through our privacy defenses. Every web platform we participate in, every detail shared on social media, every item we are sold online generates data that can be accessed by the state, whether through legal processes or less formal ones.

It is not just that these companies inculcate a culture of sharing personal information (or just taking it without meaningful consent). They also rarely design data storage systems in ways that might protect customers by limiting the data they collect, deleting it, or notifying people when it is being stored. This means that they hand it over to state authorities with little, if any, reluctance. It can be hard <*p.46*> to know where the companies end and the surveillance state begins; they share a common interest in sustaining the capitalism that overlays their relationship with users and customers. The outcome is that our digital lives are structured in ways that grant the state ever-greater powers, with ever-less accountability.

State surveillance is more heavy-handed and disciplinary, while surveillance capitalism is designed to appear consensual, convenient, diverting and mostly inconspicuous. But together they work in a complementary manner. Operating hand in glove, government and companies have created a technological ecosystem of multifunctional, cooperative surveillance.

The scale of the surveillance carried out by the Five Eyes is enormous, but it is not unprecedented. In fact, the social component of surveillance is appreciated more directly by the Chinese surveillance state than by its American counterpart. Far from trying to conceal how citizens are watched, the "Police Cloud" is a public program of information-sharing at provincial level that feeds into a national database run by the Chinese Ministry of Public Security. It uses records from people's medical history, supermarket memberships, deliveries, and other information linked to each person's national identification number. It also uses facial recognition software to identify suspected criminals. Researchers are developing a police car with a roof-mounted camera able to scan in all directions for individuals wanted by authorities. A jaywalker caught on one of the many cameras in the country (by 2020 there will be nearly half a billion) can be instantly identified as a repeat offender. These are not programs that are concealed from the public. Quite the contrary: the faces of jaywalkers, for example, are displayed on billboards above intersections in real time.

This disciplinary mode of surveillance operates in tandem with more social iterations. In 2014, the Chinese state announced its intention to implement a mandatory social credit scoring system by 2020. A trial is already underway. It is not clear which data might be used for the purposes of calculating each individual "citizen score," <*p.47*> but reports suggest a score could be sensitive to particular purchases (buying diapers, for example, rates well, but alcohol does not) and opinions of the regime expressed online, even by one's friends (critical views will lower the score). A citizen's score affects them in myriad ways, including access to credit but also travel and various free or discounted services. For the state, the social credit system represents "an important basis for comprehensively implementing the scientific development view and building a harmonious Socialist society." Those who score poorly are socially isolated, confined to the digital underclass.

Many outside observers look at this naked pursuit of total, "harmonious" social control with horror, but the similarities between the Chinese and American states are difficult to ignore. The NSA may have done it covertly, and the Ministry of Public Security may do it more openly, but the respective agencies share ambitions and outcomes. State surveillance is not simply designed to detect wrong-doing; it is about creating a more general culture of compliance and isolating resistance. It is about generating social consequences for deviant behavior. It is a version of Colquhoun's "strong and overawing hand of power" as an effective complement to the locks and bars of a prison cell. And just as Colquhoun realized, modern states are conscious that digital surveillance is a highly efficient method of enforcing a social order. Digital technology enables a scaled-up version of the pilot program that Colquhoun supervised on the Isle of Dogs over two centuries ago.

The sophisticated integration of databases by Chinese authorities represents a significant and alarming technological advance--an improved capacity to turn noise into signals, as compared to the bulk collection of the NSA. The NSA, in contrast, appears to operate on the basis that effective surveillance does not require us to think we are being watched all the time; it can be powerful by simply creating the anxiety of not knowing when we are being watched. It establishes an implied understanding that intimate information can be retrospectively pieced together at any time to tell a certain story about us. But joining these data points into a cohesive picture remains <*p.48*> the most important job for state agencies, and at this the Chinese state excels.

In contrast, the American approach has outsourced much of this work to private enterprise, and there is a lot of money to be made in this marketplace. Numerous profitable companies provide services to law enforcement using digital technology, to fill gaps in intelligence and policing. American data mining companies already boast about their capacity to generate the equivalent of a Chinese social credit score: Zest Finance, for example, at one point used the motto "All data is credit data." Major technology companies, like Google, provide data mining and analysis technology to police departments and city governments, as well as US intelligence and military agencies. Amazon is working to place voice-activated technology (such as Alexa) at the center of policing in the UK, by providing services for collating, storing and broadcasting data.

Palantir Technologies is perhaps the most notorious such company and certainly one of the most profitable in Silicon Valley. It has been described as "a darling of the US law-enforcement and national-security establishment." Palantir is tight-lipped about what it does, but its products assist organizations to easily search and find patterns in diffuse and diverse datasets. Its biggest customers are government intelligence and law enforcement agencies, and private organizations interested in fraud detection. Among other things, it is helping Immigration and Customs Enforcement (ICE) develop methods for policing immigrants in ways that are more sophisticated than ever. It is building software to help ICE access data from a range of sources across government departments that include information on foreign students, family relationships, employment, immigration history, criminal records and home and work addresses.

Palantir is doing the work that has already been done in-house at the Ministry of Public Security in China, a kind of entrepreneurial approach to shaping the capacities of public authority in a time of technological change. It was funded in its early stages by the CIA's venture capital arm, and its other notable investor is Peter Thiel, who sits on the board. "The most valuable businesses of coming <*p.49*> decades will be built by entrepreneurs who seek to empower people rather than try to make them obsolete," wrote Thiel in his book *Zero to One*. This anodyne-sounding platitude belies something darker. Thiel has made a lot of money by using technology to empower a specific class of people: those who believe that the purpose of civil government is to allow authoritarians to keep the masses in line. In reimagining techniques of social control, Thiel looks increasingly like a modern incarnation of Patrick Colquhoun, the original architect of modern policing. Just as Colquhoun pursued his thought experiments on eighteenth-century dock workers, Thiel imposes his visions on twenty-first-century netizens.

This approach to statecraft aims to diminish the potential of democracy through imposing order. "The NSA has the greatest surveillance capabilities that we've ever seen in history," said Snowden. He went on:

> Now, what they will argue, is that they don't use this for nefarious purposes against American citizens. In some ways, that's true but the real problem is that they're using these capabilities to make us vulnerable to them and then saying, "Well, I have a gun pointed at your head. I'm not going to pull the trigger. Trust me."

This capacity for surveillance, using digital technology, has created a significant concentration of power in the hands of government, not the people. Snowden refers to Americans, but the argument is true for all citizens. The concentration of power makes it harder to hold governments accountable and easier for them to persecute their enemies. It makes it harder to organize resistance to authoritarianism. It is a reminder that the Internet is not inherently or necessarily a democratic space free from government intervention.

"What made the Internet so appealing was precisely that it afforded the ability to speak and act anonymously, which is so vital to individual exploration," writes Glenn Greenwald, reflecting on the Snowden revelations. "Only when we believe that nobody else is watching us do we feel free--safe--to truly experiment, to test <*p.50*> boundaries, to explore new ways of thinking and being, to explore what it means to be ourselves." Collective private spaces--where we are able to communicate and collaborate in conditions of genuine anonymity, secrecy or both--are essential for experimentation and creativity, for exploring what we think without the ominous suspicion that we might have to pay for it later. The specter of state surveillance haunts these online spaces and diminishes our sense of what is possible, both personally and philosophically.

Snowden's revelations elicited some lofty language from lawmakers in the United States about freedom and government overreach. However, even after moderate reforms, the fundamentally unequal power relation between citizen and government has remained largely intact. The NSA still retains the power to tap into data streams collected and managed by private companies. The disturbing picture of government excess revealed by Snowden was met with a forceful claim on the part of the state that surveillance keeps us safe from terrorism. Elected representatives proved to be unreliable allies when it came to confronting the power of the surveillance state. If we want them to curb that power, we will have to find new political arguments and strategies for holding them accountable.

That is not to say that Snowden's revelations about these programs were fruitless. The mainstream adoption of encryption as standard by major technology companies is one testament to the effectiveness of his actions. (Strong encryption is increasingly under threat for this reason.) This technological response to Snowden's revelations has, at least to some degree, empowered people in many ways to resist government snooping. But individual, technical measures to resist surveillance will always be an incomplete solution. As Seda Gürses, Arun Kundani and Joris van Hoboken have argued, while the Snowden revelations were "disastrous" for the reputation of technology capitalism, "the emphasis on the technical aspects of the surveillance problem may have made the situation more manageable for them." Understanding surveillance in purely technical terms allows companies to recast the problem in ways that obscure their complicity. It reflects an ideal that "solutions for societal problems <*p.51*> can come from technical progress and sophistication"--an ideal that neatly dovetails with the objectives of technology capitalism.

We now know more than ever about how the state works to surveil us. This is a phenomenon commonly observed in self-proclaimed liberal democracies as well as in authoritarian regimes. The accepted wisdom that underpins this power dynamic is that compromises of privacy are necessary because they allow the state to guarantee our safety. The reality is altogether different. We need to scrutinize the way the state and capital work in partnership to discipline us by building bars not with steel but with silicon.

---

"Everything that can heighten in any degree the respectability of the office of *Constable*," wrote Colquhoun, "adds to the security of the State, and the safety of the life and property of every individual." Colquhoun saw it as part of his mission, in documenting the significance of the pilot program on the Thames, to highlight the importance of a corps of professionals. Only a properly paid and uniformed cohort of officers could command the respect needed to perform their job. "It cannot be sufficiently regretted that these useful constitutional officers, destined for the protection of the Public, have been ... so little regarded, so carelessly selected, and so ill supported and rewarded for the imminent risques [*sic*] which they run, and the services they perform in the execution of their duty." Respect for the police, according to Colquhoun, should arise from an acknowledgment of the public service they provide in keeping the public safe. The Thames River police were supervising the docks, but Colquhoun also sought to draw an explicit link between policing and public safety in general.

Some two centuries later, in 1996, the American sociologist David Garland observed that "one of the foundational myths of modern societies [is] the myth that the sovereign state is capable of providing security, law and order, and crime control within its territorial boundaries." According to Garland, we are taught to believe that police are an essential, inescapable element of a functioning society. Even if we may have concerns about how their power is used, or <*p.52*> want it to be reformed, the accepted wisdom is that without law enforcement there would be chaos.

These assumptions about the importance of police have gained new power in the age of digital technology. Our daily lives under surveillance capitalism involve countless algorithmic judgements made about us, defining our abstract identity. This technological capability is also used by the state, to regulate both public safety and deviance. The state collects data about us and makes use of it in discriminatory ways, and this is reflected in how the police do their job.

One place where we can see these dynamics in play is the growing use of predictive policing algorithms. Academic experiments on this kind of technology in the United States have been running for a number of years, often in partnership with industry. The basic idea is that this software tries to predict where crimes are likely to occur using a range of data sets--from traffic to the weather--to create a model for crimes such as burglaries, theft and assaults. By deploying police officers to hot spot areas, the theory is, the number of crimes will fall. In its early stages, this has appeared to meet with some success: a study from UCLA has found that predictive policing algorithms have reduced crime in the field. On the back of this kind of research, the industry that provides the relevant software products is growing quickly, with a range of consultancies selling predictive policing technology, including, predictably, Palantir. The gloss of technological precision applied to the messy work of policing is appealing, as is the idea that such programs can lighten the workloads of police forces and relieve some of the pressure on their budgets.

It remains unclear exactly how these models work--they are proprietary and therefore secret--but we know that one of the primary data inputs fed into these algorithms is crime statistics. If a burglary happens in one house, statistically that may affect the neighboring houses' risk of also being burgled; if a person is arrested for selling drugs in a particular place, past crime data may suggest that the location is a hot spot for dealing.

The problem is that crime statistics do not reflect the crimes <*p.53*> actually occurring; rather, they provide a picture of the state's response to crime. By relying on this data, the software has a tendency to target low-income communities and minority neighborhoods, even though health data and population models indicate that drug use is similar across race and income groups. Similar concerns exist with stop-and-frisk policies, which have documented biases, more specifically based on race, which are then fed into the algorithm. Facial recognition databases also contribute to the problem. Law enforcement agencies hold records on millions of people for this purpose, largely unregulated. There are serious, documented issues with the accuracy of facial recognition software, particularly for people who are not white. Given that minority communities are overpoliced already, they end up overrepresented in these databases. It entrenches the criminalization of nonwhite communities, importing this discrimination into our digital future.

These biased data sets and algorithms, when used in a law enforcement context, have significant consequences. They generate a feedback loop shaped by racism and institutionalize certain understandings of risk. Both of these serve to confirm and exacerbate discrimination through the oversampling people who are already discriminated against, generating even more biased data that justifies further discrimination. Rather than some objective pursuit of criminality, these algorithms lead to confirmation of a historical tendency on the part of police to focus on poor people and racial minorities. Public notions of who commits crimes and what neighborhoods are safe become distorted.

The data on which we train technology "uncritically ingests yesterday's mistakes," as James Bridle puts it, encoding the barbarism of the past into the future. It is a history that is carried forward uncritically. The perception that these algorithms are scientific and objective goes unchallenged. This gives them a powerful allure for both industry and the state. Industry understands that there is money to be made in the promise of enhancing policing technology; the state appreciates the distance it puts between law enforcement and allegations of bias.

<*p.54*> Many technologists and data scientists prefer a blunt metaphor for this use of algorithms: garbage in, garbage out. The discriminatory social trends that have already been exposed in everyday policing will continue to be reproduced in the supposedly more objective and scientific methodology of computerized predictive policing. Over time we can expect more data sets to be input, to better refine the algorithms. But given the biased way in which these algorithms are built and deployed, this is unlikely to actually produce a safer society; it will just let the police work more efficiently and effectively, which is not the same thing.

If these programs were really about safety, the algorithms might look quite different. They might target white-collar crime, for example. In fact, that could be a more suitable use for the technology. Despite being a complex phenomenon that resists simple definitions, academic research suggests that white-collar crime often follows clear and predictable patterns, particularly around the industries in which it tends to occur. Yet this kind of criminality, despite being highly damaging and widespread, is not the focus of predictive policing.

The use of digital technology by police reveals how ideas of deviance and safety are not objectively determined but are socially produced. Much in the same way that the eighteenth-century Marine Police Office created specific ideas of criminality through its work to prevent crime, digital technology is finding new ways to criminalize classes of people who are already oppressed. How the police decide what is "criminal" involves an array of decisions and policies that have been influenced by discriminatory social practices, reflected and reproduced in technological tools.

We can also see this phenomenon in action, perhaps with even more intensity, in American counterterrorism policy. The risk of harm as a result of terrorism is vanishingly low for most Americans, but the amount spent on policing that threat is disproportionately high. In other words, what it means to be safe is not necessarily about reducing the most pressing threats of harm, but rather specific threats that are politically determined. For example, documents <*p.55*> revealed by Snowden set out the Skynet program run by the NSA, which uses an algorithm to identify terrorists. This algorithm was developed using data about "known terrorists" and comparing it with a wide range of behavioral data taken from cell phone use. The algorithm's highest-rating target was a man named Ahmad Zaidan. The NSA documents prominently, and with apparent confidence, label Zaidan a member of Al Qa'ida.

But Zaidan is not a terrorist. At the time, he was the Al Jazeera bureau chief in Islamabad. While Zaidan may have met with known terrorism suspects, traveled with them and shared social networks, he did this as part of his job as a journalist. While Zaidan may be a perfect fit for the algorithm to identify terrorists, it is immediately obvious to any human that he did not belong in this category at all.

To outside observers, the material about Skynet looks absurd to the point of being comical. But Zaidan himself offered a more sober assessment: "The allegations against me put my life in clear and immediate danger, when we consider that many people have lost their lives as a result of such fake information." These programs have real-world consequences: thousands of people have been targeted by lethal drone strikes conducted by US agencies over the last two decades. It is arguable that some, if not many, were like Zaidan and posed no threat to the lives of innocent people. This practice of data modeling, connected by government agencies to our real-world identity, puts us all at risk of arbitrary violence by the state. This technology legitimates unaccountable use of weaponized robots under the cover of a public safety project. It may not be wholly intentional; the NSA can hardly have wanted the kind of false positive generated by Skynet (though it is important to remember that President Bush discussed targeting Al Jazeera militarily in 2005, and the US military has been accused of bombing its buildings deliberately). But the capacity to treat people in this way--as unpeople--reveals both the dehumanizing ideology that informs certain kinds of foreign policy project and the power of technology to achieve those ends. The objectification of people by algorithms is a problem that will be explored further in the next chapter.

<*p.56*> The Skynet program shows how technology accelerates the power of law enforcement to use violence arbitrarily on the disenfranchised and powerless. The recurring theme is that state agencies that do policing work create a specific idea of criminality and then use digital technology to "protect" us based on the fear they have engendered. It would be a mistake to see Skynet simply as an example of sloppy data science. It is a tiny insight into the myriad and secretive ways the state uses violence, guided by technology, for social control.

I do not mean to deny the existence of real threats to people's safety, from individual acts of interpersonal violence to the political violence of terrorism. Rather, my argument is that digital technology is being used for a more specific purpose, namely to substantiate the state's claim to be the great provider of safety, even when it does not achieve this objective. It feeds on a culture of what Adam Greenfield calls "unreconstructed logical positivism": a belief that the world is perfectly knowable, and that, with the correct inputs and algorithms, technical systems can generate solutions to all collective human needs. It also relies on what James Bridle calls "automation bias," our trust of machines to generate trustworthy responses because the computational processes are too complex and opaque to allow us space for criticism. By generating technical solutions to the problem of crime, the myth of the state as the indispensable provider of safety becomes a logical, indisputable truth. It creates a category of people it classifies as criminals, which it then polices and trains us to fear. This kind of statecraft has broad implications: it downplays other threats to our safety, glosses over the social causes of crime, and discourages us from thinking about ways in which digital technology might contribute to minimizing genuine threats.

How can we change this situation? It will require us to redefine what it means to be safe, and to imagine alternatives to the police. A safe society is not one that strips away the social security net and uses increased numbers of police to manage the fallout. We need to reject the neoconservative perspective, as Alex Vitale puts it, "that ses all social problems as police problems" and appoints them to be the lead agency in dealing with social, economic and political problems, <*p.57*> armed with cutting-edge technology. We cannot let technology be used to upgrade the socially destructive phenomenon of modern policing.

We also need legal requirements for transparency in the state's use of algorithms in public decision-making. Unless the logic and data inputs in predictive police algorithms are publicly available, fundamental rights within the criminal justice system will be eroded. Such transparency will allow courts to test the reliability and accuracy of such programs and ensure they avoid arbitrary interferences with private life. It is true that these systems can be mind-bogglingly complex, with so many data inputs and variables that discovering their biases and appreciating their nuance can verge on impossible. But this is not a novel problem, and it is manageable. It is possible to imagine a statutory list of assumptions and data inputs that can be incorporated into this technology, and a similar list of prohibitions. The insurance industry is already regulated in some places in this way, allowing some factors to affect pricing of products and prohibiting others from doing so. Public participation in designing such rules will be necessary and valuable. If the makers of these programs lack the transparency to allow public scrutiny, we should stop using the programs.

We also need greater transparency over intelligence agencies. Oversight by regulatory bodies, reporting requirements, and better protection for whistleblowers who expose wrongdoing are some obvious steps toward this objective. We expect that in societies ruled by law, authorities will obtain a warrant before entering our private homes, so why not also require this for our private online spaces? The right to a private domain, free from government interference, has always been a right that needed fighting for. A similar struggle will be required today.

But it is also worth thinking a little more radically about the possibilities of technology to reduce harm. We need to decouple algorithms used to map trends in harmful behavior from law enforcement. It is possible to imagine a world where this type of data analysis is used to inform government spending and social <*p.58*> programs, for example. A good example of where to start might be the Ceasefire program, which aims to prevent interpersonal violence through social intervention:

> Under Ceasefire, police teamed up with community leaders to identify the young men most at risk of shooting someone or being shot, talked to them directly about the risks they faced, offered them support, and promised a tough crackdown on the groups that continued shooting. In Boston, the city that developed Ceasefire, the average monthly number of youth homicides dropped by 63 percent in the two years after it was launched.

Rather than focusing on restricting gun purchase and ownership, which has largely proven ineffective, or randomized stop-and-frisk practices, which confirm police biases, a program like Ceasefire uses social relationships and community authority to address harmful behavior. There are all sorts of ways in which data and thoughtful algorithms could inform social interventions and the provision of services to try to de-escalate interpersonal crime, resolving many issues without the need for police intervention at all.

It is important that this kind of community work does not become a justification for more surveillance. Ceasefire has, for example been implemented in New Orleans in less desirable ways. In 2013, Palantir used the city as a testing ground for its intelligence products focused on predictive policing. The program was rolled out almost covertly, with minimal oversight by elected officials. It was reportedly designed to work in partnership with the Ceasefire program, to ensure that community engagement preceded law enforcement involvement. But the emphasis ended up on prosecutions much more than on community engagement, ultimately undermining the intervention work. "It's supposed to be ran [*sic*] by people like us instead of the city trying to dictate to us how this thing should look," said Robert Goodman, a New Orleans native who worked on the Ceasefire program. "As long as they're not putting resources into the hoods, nothing will change."

<*p.59*> The ideas that underpin Ceasefire start in the right place, focusing on empowering local activists and resourcing them to intervene into cycles of violence as peers rather than enforcers. But these ideas will only be effective, and make effective use of technology, if they resist becoming a cover for increased police power. Good policy design, with community involvement and accountability and drastic limits on police powers, can minimize this risk. In these contexts, it becomes possible to reimagine an idea of safety built on community practices that make use of technology in ways that are designed to reduce social isolation and risk. Such social interventions undermine the very need for a specialized police force.

Digital technology can help make our societies safer--it can reduce crime and protect us from other harmful behavior. But such outcomes are by no means guaranteed if we hand over powerful tools, developed in the digital ae, to the police. "We are developing an official criminology that fits our social and cultural configuration," Garland warned two decades ago, "one in which amorality, generalized insecurity and enforced exclusion are coming to prevail over the traditions of welfare-ism and social citizenship." In other words, Garland was observing how a culture of fear was beginning to take precedence over ideas of collective responsibility for social problems over two decades ago. He has been proven more correct than he would have wished.

---

Modern law enforcement and intelligence agencies are established and sophisticated well beyond the wildest imaginings of Harriott, Colquhoun and Bentham at the end of the eighteenth century. In their day, these men faced "infinite difficulties and discouragements" as they strove to get a professional police force up and running to suppress "the extensive and enormous evils" that plagued their society--and convince civil government that such an approach would work. But the practical application of their ideas--particularly in the digital age--has generated its own valency. Agencies charged with the protection of the public have ended up serving their own interests above all. When law enforcement and intelligence agencies <*p.60*> are so powerful that they sit above organs of democratic governance, it is almost impossible to hold them accountable. This creates a situation where public safety is not simply neglected or selectively defined; it creates a situation where public safety is put at risk.

In May 2017, cyberattacks affected 200,000 computers in 150 countries. This came with enormous human consequences, affecting universities and health systems. The attacks exploited a zero-day vulnerability in Microsoft software. A zero-day vulnerability is like finding a secret gate into a walled garden: it gives you access to do things you should not be able to do, but only for as long as the point of access remains undetected. Zero-day vulnerabilities are not intentionally placed there by programmers--rather they represent weaknesses or holes in the coding. The name "zero-day" indicates that the problem is still undetected by the person running the program, that is, the vulnerability has been known about for zero days. There is a whole market for finding these vulnerabilities and fixing them, and software companies spend a lot of time looking for them and sending out updates to patch them. Other hackers do it for pay. In this case, the vulnerability allowed hackers to lock the user out of the machine and everything on its hard drive unless a ransom was paid.

It seems that whoever launched this attack, known as the WannaCry worm, exploited a zero-day vulnerability that Microsoft had become aware of a few weeks before through the NSA. The NSA had seemingly found this vulnerability at some point before the attacks, possibly five years earlier, though we know few specifics. Rather than disclosing it to Microsoft to fix, the NSA kept it secret, as part of its efforts to accumulate a kind of digital arsenal; so long as it remained unpatched, they could exploit it for their own intelligence purposes. The problem was that at some point it was apparently lost or stolen, forcing the NSA to tell Microsoft about its existence.

This was not the first time the NSA had been slow to share its knowledge of security problems with widely used software. The Heartbleed vulnerability was a bug in encryption software used by <*p.61*> websites that allowed attackers to eavesdrop on communications and steal data directly from services and users, including by impersonating them. When it was discovered in 2014, there were estimates that up to two-thirds of the world's websites were exposed, leaving large amounts of confidential information available on the Internet and allowing attackers to easily steal this information without leaving a trace. It had been identified by the NSA two years earlier.

As Microsoft pointed out in the wake of WannaCry, it was the equivalent of a Tomahawk missile going missing. Or like the theft of the only remaining stockpiles of smallpox, which have been hoarded by Russia and the United States. Or someone stealing a nuclear warhead.

When he publicly discussed these cyberattacks, Snowden touched on how this kind of statecraft is predicated on the idea that the price we have to pay for security is our right to privacy. For safety's sake, we have to accept that police and spooks will know all about us. We have to accept that our software will be full of secret gates into our private gardens, because it serves the purposes of security agencies. The way these cyberattacks unfolded proved how false this bargain is:

> This has never been a conversation about privacy vs. security. Because privacy and security improve together ... they are actually tied to each other. When one is reduced, the other is reduced. Surveillance and privacy are the contradictory factors. When surveillance increases, privacy decreases.

The NSA's idea of safety is different from how everyday citizens might understand the concept. The NSA seeks to accumulate digital weaponry and surveillance power at the expense of our individual freedom and privacy.

Moreover, what is at risk is much bigger than individual freedom and privacy. In 2012, Bruce Schneier observed how we are "in the early years of a cyberwar arms race. It's expensive, it's destabilizing, and it threatens the very fabric of the Internet we use every day." <*p.62*> The accumulation of digital weaponry--technological tools that allow spying or hacking of systems or other violations of digital infrastructure--is often a goal that takes precedence over privacy and security for the surveillance state. The kind of cyber-militarism that Schneier alluded to has only become worse in subsequent years. As more of our personal lives, social services and public infrastructure become integrated with digital technology, power shifts toward institutions that can control these systems, either legitimately or through nefarious means.

Cyber-militarism--and the tussle for power it represents--does not actually make us safer, it exposes us to risks. For some sections of society, which rely on the state to serve and protect their interests, the risks are worth taking. Those sections are much the same as thew were in the days of Harriott and Colquhoun: they are those with wealth. The consequences of any risks, when they materialize, an often borne by everyday citizens.

Imagine, for example, if there was a zero-day vulnerability identified in the software used in a certain make of autonomous car or a weapon or an interface for a public transit system or power grid. If the NSA had some intelligence advantage to gain by keeping this vulnerability secret, it is hard to imagine them trying to patch it. This practice leaves us all exposed--a risk that only becomes more widespread and complex as more software gets incorporated into everyday products and systems.

The state has a long history of spearheading technological development, marshaling public resources to keep ahead of national rivals and to support domestic private enterprise. This work also feeds back into its objective to preserve the status quo. It discourages the idea that alternatives are possible, by encouraging us to internalize the feeling of being watched. As the history of the police shows, public and private power are often tangled together in ways that are deep-seated and mutually reinforcing. Despite nominally representing the public, the state in many ways serves the interests of the propertied classes first, reflecting the history of the London docks during the industrial revolution, and the police who began to surveil them.

<*p.63*> It is time to shift this paradigm and start to think about ways in which digital technology can make our society more secure, in ways that are complementary with the freedom that comes with privacy. Data and computing can be deployed in ways that address social harms and tackle the problems that lead to violence. Whistleblowers like Edward Snowden are as much a constant of history as the bullies and thugs they expose. They are the surveillance agency of the people; they are the undercover spies for the powerless, and there are always more of them. We should protect them when they step forward, listen to what they have to say, use what we learn to hold power accountable, and never presume that this will happen automatically. Their presence is not a sufficient check on those in power. Rather, our job is to carry on the work they have started.

What began on the London docks two centuries ago was an experiment in state-sanctioned force and suppression for the purpose of protecting private property, justified by reference to public safety. We need to redefine what it means to be safe, and reexamine the threats posed to our collective security by unaccountable organs of state power. We need to break the locks and bars created by state surveillance, before we find ourselves in a digital dystopia.

Nearly a century after the establishment of the Marine Police Office, the London docks experienced a social transformation of a different kind. In 1889, the dock workers went on strike over their miserable pay. Other laborers joined them, until 130,000 people shut down the vital arteries of the city, in what came to be known as the Great London Dock Strike. Those on strike faced serious obstacles, but their campaign survived on the support from fellow workers as far away as Australia. "The proverbial small spark has kindled a great fire which threatens to envelop the whole metropolis," declared a news report. As the threat of a general strike grew, the dock companies relented, and ultimately the workers won their demands. It represented a historic reclamation of their power as a class of wage-earners, achieved by withdrawing their labor. The strike became a turning point in British labor history, a moment in which a new imagining of the potential of workers' power was <*p.64*> brought into relief and set the paradigm for a bold and broad approach to labor organizing over the next century.

More than a century on, the moment is ripe for another shift in the balance of power, away from those who surveil and seek to control us, and toward those who seek to rekindle a society built on solidarity.
